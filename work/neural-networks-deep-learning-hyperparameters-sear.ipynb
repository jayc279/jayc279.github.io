{"metadata":{"colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/gdrive', force_remount=True)\n# !pip install -q kaggle\n# !ls ~/.kaggle/kaggle.json\n# !ls gdrive/MyDrive/colab_work/kaggle.json\n\n# !mkdir -p ~/.kaggle\n# !cp gdrive/MyDrive/colab_work/kaggle.json ~/.kaggle/\n# !chmod 600 ~/.kaggle/kaggle.json\n","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Neural Networks Deep Learning Hyperparameters search on Titanic Disaster dataset","metadata":{}},{"cell_type":"markdown","source":"@misc{omalley2019kerastuner\n,     title        = {KerasTuner}\n,     author       = {O'Malley, Tom and Bursztein, Elie and Long, James and Chollet, Fran\\c{c}ois and Jin, Haifeng and Invernizzi, Luca and others}\n,     year         = {2019}\n,     howpublished = {https://github.com/keras-team/keras-tuner}\n}","metadata":{}},{"cell_type":"markdown","source":"{cite}`omalley2019kerastuner,\ntitle:KerasTuner,\nauthor:O'Malley, Tom and Bursztein, Elie and Long, James and Chollet, Fran\\c{c}ois and Jin, Haifeng and Invernizzi, Luca and others,\nyear:2019,\nhowpublished:https://github.com/keras-team/keras-tunerqiime","metadata":{}},{"cell_type":"markdown","source":"### Jupyter Notebook [uploaded to GitHub](https://github.com/jayc279/kaggle_notebooks/blob/main/hyper_parameters_tuning_titanic-disaster_data.ipynb)\n[Titanic Disaster dataset using hyperparameters captured from run executed above Notebook](https://github.com/jayc279/kaggle_notebooks/blob/main/titanic-ml-from-disaster-neural-networks.ipynb)","metadata":{}},{"cell_type":"markdown","source":"## import packages","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport math\nimport os\nimport gc\nimport re\nimport inspect\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom math import floor\nfrom math import ceil\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', None)","metadata":{"id":"En5gBHrSJ-lU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import cross_val_score","metadata":{"id":"Evs1xONSKQkv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow.keras as keras\n\nfrom keras.layers import Dense, Dropout, Flatten, BatchNormalization, Input\nfrom keras.models import Sequential\nfrom keras.optimizers import Adadelta, Adagrad, Adam, Adamax, Nadam, SGD, RMSprop, Ftrl\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.layers import LeakyReLU, ReLU, ELU, PReLU\nfrom keras.constraints import MaxNorm\nfrom keras.initializers import glorot_uniform, he_uniform, glorot_normal, he_normal\nfrom keras.initializers import uniform, normal, zero\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### TPU - GPU setup","metadata":{}},{"cell_type":"code","source":"# Detect TPU\ndef tpu_setup():\n    try:\n        tpu = tf.distribute_cluster_resolver.TPUClusterResolver()\n        print('detected: ', tpu.master());\n    except ValueError as e:\n        print('Error: no TPU: ', e)\n        tpu = None\n\n    if tpu:\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    else:\n        strategy = tf.distribute.get_strategy()\n\n    return strategy\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def gpu_setup():\n    # Detect GPUs, return appropriate strategy\n    gpus = tf.config.experimental.list_physical_devices('GPU')\n    \n    if gpus:\n        try:\n            # setup up GPU memory growth as True for each GPU\n            for gpu in gpus:\n                tf.config.experimental.set_memory_growth(gpu, True)\n            strategy = tf.distribute.MirroredStrategy()\n            print(f'using: {len(gpus)}')\n        except:\n            strategy = tf.distribute.get_strategy()\n            \n        return strategy\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Setting TPU or GPU strategy for this session\n# strategy = tpu_setup()\n# strategy = gpu_setup()\n# print('Replicas in strategy: ', replicas)                 # object has no attribute 'num_replicas_in_sync'\n# !nvidia-smi","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function to clear model and garbage collector\ndef clear_bags(model):\n  del model\n  gc.collect()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### load and clean Titanic Disaster dataset","metadata":{}},{"cell_type":"markdown","source":"load Ttitanic Disaster train and test data. you will see we have:  \nfor training: 891 rows and 12 columns  \nfor testing: 418 rows and 11 columns (does not include `Survived` column)","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/titanic/train.csv')\ntest_df = pd.read_csv('/kaggle/input/titanic/test.csv')\nprint(train_df.shape, test_df.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Split `PassengerId` column from train dataset and create `true` label dataset - y_train  \nRemove columns that do not add to predictions - columns that do not affect Survival outcomes  \nColumns like: `PassengerId`, `Name`, `Ticket`, `Fare`  \nDrop same columns as above from `test` dataset\nDrop `Survived` since we already created `y_train` dataset","metadata":{}},{"cell_type":"code","source":"y_train = train_df['Survived']\ntrain_df.drop(['PassengerId', 'Survived','Name','Ticket','Fare'], axis=1, inplace=True)\ntrain_df.shape, y_train.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"passengerId = test_df['PassengerId']\ntest_df.drop(['PassengerId', 'Name','Ticket','Fare'], axis=1, inplace=True)\ntest_df.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### column `Age` clean-up","metadata":{}},{"cell_type":"code","source":"train_mean_age = train_df['Age'].mean(numeric_only=True)\ntest_mean_age = test_df['Age'].mean(numeric_only=True)\ntrain_quant_age = train_df['Age'].quantile(0.75)\ntest_quant_age = test_df['Age'].quantile(0.75)\nprint(train_mean_age, train_quant_age)\nprint(test_mean_age, test_quant_age)\n\nma = train_mean_age\nmb = train_quant_age\nif train_mean_age > train_quant_age:\n    mb = ma\n    ma = train_quant_age\n\ntrain_df['Age'] = train_df.apply(\n    lambda row: np.random.randint(ma, mb) if np.isnan(row['Age']) else row['Age'], axis=1\n)\ntest_df['Age'] = test_df.apply(\n    lambda row: np.random.randint(ma, mb) if np.isnan(row['Age']) else row['Age'], axis=1\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### column `Cabin` clean-up","metadata":{}},{"cell_type":"code","source":"# Regression runs\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n\n# fill NaNs in both Train and Test data with 'L0' - lowest leverl in Titanic decks\ntrain_df['Cabin'] = train_df['Cabin'].fillna('L0')\ntest_df['Cabin'] = test_df['Cabin'].fillna('L0')\n\n# train_df['Cabin'] = train_df['Cabin'].map(lambda x: re.sub(\"\\d+\",\"\", x))\ntrain_df['Cabin'] = train_df['Cabin'].map(lambda x: re.sub(\"\\s+\",\"\", x))\ntrain_df['Cabin'] = train_df['Cabin'].map(lambda x: re.sub(r'(.)\\1+', r'\\1',x))\n\n# test_df['Cabin'] = test_df['Cabin'].map(lambda x: re.sub(\"\\d+\",\"\", x))\ntest_df['Cabin'] = test_df['Cabin'].map(lambda x: re.sub(\"\\s+\",\"\", x))\ntest_df['Cabin'] = test_df['Cabin'].map(lambda x: re.sub(r'(.)\\1+', r'\\1',x))\n\nX1 = list(np.asarray(train_df['Cabin'].unique()))\nX2 = list(np.asarray(test_df['Cabin'].unique()))\nX3 = X1 + X2\n\n# LabelEncoder().fit(X3)\nle = LabelEncoder()\nle.fit(X3)\n\ntrain_df['Cabin'] = le.transform(train_df['Cabin'])\ntest_df['Cabin'] = le.transform(test_df['Cabin'])\n# train_df['Cabin'][:50]\n# test_df['Cabin'][:50]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### one-hot encoding of categorical columns","metadata":{}},{"cell_type":"code","source":"train_df = pd.get_dummies(train_df)\ntest_df = pd.get_dummies(test_df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"one last check to make sure columns in training and test datasets match   \nif columns do not match we can raise exception or drop the offending columns and continue  \n**Note** if the number of columns between train and test sets do not match, do not proceed\n\nhere the code drops mis-matched columns and continues with hyperparameter tuning","metadata":{}},{"cell_type":"code","source":"# before split drop columns that don't match between train and test\nfor i in train_df.columns:\n  # print(f'column:-{i}-')\n  if i not in test_df.columns:\n    print(f'column:-{i}- does not exist in test_df')\n    train_df.drop(i, inplace=True, axis=1)\n\nfor i in test_df.columns:\n  # print(f'column:-{i}-')\n  if i not in train_df.columns:\n    print(f'column:-{i}- does not exist in train_df')\n    test_df.drop(i, inplace=True, axis=1)\n\nprint(train_df.shape, test_df.shape, y_train.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"convert `True` to `1` and `False` to `0`   \nThis is a requirement to process data in Neural Networks","metadata":{}},{"cell_type":"code","source":"# convert True/False to 1 & 0\ntrain_df = train_df.replace({True: 1, False: 0})\ntest_df = test_df.replace({True: 1, False: 0})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"scale data using SkLearn StandardScaler    \nPlease use other scalers if you think it might improve the model\n\nwe only scale two columns in both train and test datasets  \n1. Age\n2. Cabin","metadata":{}},{"cell_type":"code","source":"sc = StandardScaler()\ntrain_df['Age_scaled'] = sc.fit_transform(train_df[['Age']])\ntest_df['Age_scaled'] = sc.transform(test_df[['Age']])\ntrain_df.drop(['Age'], inplace=True, axis=1)\ntest_df.drop(['Age'], inplace=True, axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.shape, y_train.shape, test_df.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# scale 'Cabin' as well \ntrain_df['Cabin_scaled'] = sc.fit_transform(train_df[['Cabin']])\ntest_df['Cabin_scaled'] = sc.transform(test_df[['Cabin']])\ntrain_df.drop(['Cabin'], inplace=True, axis=1)\ntest_df.drop(['Cabin'], inplace=True, axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## check data types of train and test - except for Age, all should be categorical\ntrain_df.shape, y_train.shape, y_train.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"split trainingset into training and validation datasets.  \nOptimal woule be if we did not have to split training set to create validation dataset\n\nprint shape to make sure:  \n1. number of columns match between train and test datasets\n2. number of rows match between train and validation datasets\n","metadata":{}},{"cell_type":"code","source":"# split train dataset into train and validation datasets\n# train_df_copy = train_df.copy()\ntrain_X, val_X, train_y, val_y = train_test_split(train_df, \n                                                  y_train, \n                                                  test_size=0.3, \n                                                  random_state=42,\n                                                 shuffle=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_X.shape, val_X.shape, train_y.shape, val_y.shape)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<hr><br>\n<hr>","metadata":{}},{"cell_type":"markdown","source":"### Neural Networks - Deep Learning Hyperparameter Tuning","metadata":{}},{"cell_type":"code","source":"seed = 7\ntf.random.set_seed(seed)\n\n# create dictionary to store hyper-parameters\nhyper_params = dict()","metadata":{"id":"7ZlhviYSOGGj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Declare defaults to setup up Deep Learning Neural Network to hypertune parameters","metadata":{}},{"cell_type":"code","source":"def_batch = 32\ndef_epoch = 100\ndef_num_layers = 4\ndef_dropout = 0.2\ndef_neurons = math.ceil(train_X.shape[1] * def_batch)\n\nprint(\n    f\"{'def_neurons:':<15}{def_neurons:>10}\",\n    f\"{'def_num_layers:':<15}{def_num_layers:>10}\",\n    f\"{'def_batch:':<15}{def_batch:>10}\",\n    f\"{'def_epoch:':<15}{def_epoch:>10}\",\n    sep='\\n'\n)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **create Deep Learning Neural Netowrk model to Tune hyper-parameters**","metadata":{"id":"ULu0N86PVIaG"}},{"cell_type":"code","source":"# default values to setup a Keras model\nhyper_params['learning_rate']     , best_learning_rate       = None, None  # 0.001\nhyper_params['momentum']          , best_momentum            = None, None  # 0.1\nhyper_params['init_mode']         , best_init_mode           = None, None # 'he_normal'\nhyper_params['activation']        , best_activation          = 'relu', 'relu'\nhyper_params['dropout']           , best_dropout             = None, None  # 0.2\nhyper_params['weight_constraint'] , best_weight_constraint   = None, None  # 4.0\nhyper_params['neurons']           , best_neurons             = def_neurons, def_neurons  # 256\nhyper_params['optimizer']         , best_optimizer           = 'Adam', 'Adam'  # 'Adam'\nhyper_params['epochs']            , best_epoch_num           = def_epoch, def_epoch\nhyper_params['batch_size']        , best_batch_size          = def_batch, def_batch\nhyper_params['num_hidden_layers'] , best_num_hidden_layers   = 3, 3\n\n# input dataframe\ninputdf=train_df         # define the dataframe to use for input_shape - Neural Networks only need number of features\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_hyper_model(lr=best_learning_rate,\n                       momentum=best_momentum,\n                       init_mode=best_init_mode,\n                       activation=best_activation,\n                       dropout=best_dropout,\n                       weight_constraint=best_weight_constraint,\n                       neurons=best_neurons,\n                       optimizer=best_optimizer,\n                       num_hidden_layers=best_num_hidden_layers,\n                      inputdf=inputdf):\n    \n    # clear model session and recreate - \n    # according to Keras docs works for a functional model\n    keras.backend.clear_session()\n    dense_dict = inspect.signature(Dense)\n    dropout_dict = inspect.signature(Dropout)\n    batch_norm_dict = inspect.signature(BatchNormalization)\n    \n    ###################################################\n    # get defaults for hyper-search parameter values\n    ###################################################\n    # ValueError: Unknown initializer: 'glorot_uniform'. \n    # Please ensure this object is passed to the `custom_objects` argument.\n    if init_mode is None:\n        k = 'kernel_initializer'\n        # val = str(dense_dict.parameters[k]).split('=')[1]\n        init_mode = str(dense_dict.parameters[k]).split('=')[1] \n        init_mode = 'glorot_normal'\n\n    if activation is None:\n        k = 'activation'\n        # val = str(dense_dict.parameters[k]).split('=')[1]\n        activation = str(dense_dict.parameters[k]).split('=')[1] \n         \n    if weight_constraint is None:\n        k = 'kernel_constraint'\n        # val = str(dense_dict.parameters[k]).split('=')[1]\n        # weight_constraint = str(dense_dict.parameters[k]).split('=')[1] \n        weight_constraint = 4.0\n\n    if lr is None:    # learning_rate - optimizer\n        lr = 0.001\n    \n    if momentum is None:\n        k = 'momentum'\n        # val = str(dense_dict.parameters[k]).split('=')[1]\n        momentum = np.float32(str(batch_norm_dict.parameters[k]).split('=')[1])\n        # momentum = 0.1\n\n    if dropout is None:\n        dropout = 0.2\n\n    ###################################################\n    # function to return a list of layers depending on the \n    # number asked and number_of_neurons in first layer\n    ###################################################\n    def return_num_layers(nn_num=neurons, n_layers=num_hidden_layers):\n        layers = []\n\n        divn = 1 if n_layers <= 1 else (n_layers -1)\n        # first layer to contain all neurons\n        first_lyr_units = neurons\n\n        # last layer (before output) is:\n        last_lyr_units = neurons // n_layers\n        \n        incr_by = (last_lyr_units - first_lyr_units) / divn\n        num_neurons = first_lyr_units\n        \n        # print(f'num_neurons:{num_neurons} - incr_by:{incr_by}')\n        for i in range(1, n_layers + 1):\n            layers.append(round(math.ceil(num_neurons)))\n            # print(f'layer:{i} - num_neurons:{num_neurons} - layers:{layers}')\n            num_neurons = num_neurons + incr_by\n\n        return layers\n\n    \n    # neurons = math.ceil(neurons)\n    neurons = math.ceil(inputdf.shape[1] * (best_batch_size / 2) )\n    num_lyrs = return_num_layers(neurons, num_hidden_layers)\n    # print('num_lyrs ', num_lyrs)\n    \n    ###################################################\n    # create model\n    ###################################################\n    model = Sequential()\n    # model.add(Input(shape=(inputdf.shape[1],)))   # used in functional\n\n    model.add(Dense(num_lyrs[0] * 2, input_shape=(inputdf.shape[1],), \n                    kernel_initializer=init_mode, activation=activation,\n                    kernel_constraint=MaxNorm(weight_constraint)))\n\n    if momentum > 0.5:\n        model.add(BatchNormalization(momentum=momentum))\n\n    if len(num_lyrs) > 2:\n        for i in range(1, num_hidden_layers -1):\n            model.add(Dense(num_lyrs[i -1] * ( i / num_hidden_layers ), \n                            kernel_initializer=init_mode, activation=activation,\n                            kernel_constraint=MaxNorm(weight_constraint)))\n    \n    # model.add(Dense(neurons//2, kernel_initializer=init_mode, activation=activation,\n    #               kernel_constraint=MaxNorm(weight_constraint)))\n\n    if dropout > 0.1:\n        model.add(Dropout(dropout))\n\n    # model.add(Flatten())\n    if len(num_lyrs) > 1:\n        model.add(Dense(num_lyrs[-1], kernel_initializer=init_mode, activation=activation,\n                        kernel_constraint=MaxNorm(weight_constraint)))\n    \n    model.add(Dense(1, kernel_initializer=init_mode, activation='sigmoid'))\n    ###################################################\n    \n    # Compile model\n    opt=keras.optimizers.get(optimizer)   # opt.__dir__ to know what can be set\n    opt.learning_rate=lr\n    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### create model (default settings) before hyper-parameter tuning to check current layers and parameters","metadata":{}},{"cell_type":"code","source":"cvmodel = create_hyper_model()\ncvmodel.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Tune hyperparameters**","metadata":{}},{"cell_type":"markdown","source":"if you have the resources you can tune on as many hyperparmaters as you wish  \nfor this Notebook only tuned on a few with limited sets of inputs fo each parameter","metadata":{}},{"cell_type":"code","source":"clear_bags(cvmodel)\n# cvmodel = KerasClassifier(build_fn=create_hyper_model, verbose=0)\n\n# cvmodel = KerasClassifier(build_fn=create_hyper_model, verbose=0)\n\nnum_hidden_layers = [2, 3, 4, 5]\nbatch_size=[16, 32, 64]\nepochs = [30, 50, 70, 90]\nneurons = [128, 256, 512]\n\n# optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\noptimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adam']\n\nlearn_rate = [0.001, 0.01, 0.1]\nmomentum = [0.0, 0.2, 0.4, 0.6]\n\n# init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\ninit_mode = ['glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n\n# activation = ['relu', 'tanh', 'linear', 'selu', 'elu', 'leaky_relu', 'exponential','gelu','mish']\nactivation = ['relu', 'tanh', 'leaky_relu']\n\nweight_constraint = [0.1, 1.0, 2.0]\ndropout_rate = [0.3, 0.5, 0.6, 0.7]\n\ncvmodel = KerasClassifier(build_fn=create_hyper_model,\n                          neurons = neurons,\n                          num_hidden_layers = num_hidden_layers,\n                          # optimizer = optimizer,\n                          # lr = learn_rate,\n                          # momentum = momentum,\n                          # init_mode = init_mode,\n                          # activation = activation,                          \n                          # weight_constraint = weight_constraint,\n                          dropout = dropout_rate,\n                          epochs= epochs,\n                          batch_size = batch_size,\n                          inputdf=train_df,\n                          verbose=0,\n                          )\n\n# create param_grid dictionary\nparam_grid = dict(batch_size=batch_size,\n                  # optimizer=optimizer,\n                  # lr=learn_rate,\n                  neurons=neurons,\n                  num_hidden_layers = num_hidden_layers,\n                  # momentum=momentum,\n                  # init_mode=init_mode,\n                  # activation=activation,\n                  # weight_constraint=weight_constraint,\n                  dropout=dropout_rate,\n                  epochs=epochs)\n","metadata":{"id":"JyXbIdDkOYNE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# GridSearchCV on all hyper-parameters\ngrid = GridSearchCV(estimator=cvmodel, param_grid=param_grid, n_jobs = 1, cv = 3)\ngrid.fit(train_df,y_train)","metadata":{"id":"PDSA1G7ZOdC6","outputId":"031ab1f4-7027-4966-f9a3-b1345e48b9a7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'best_fit:{grid.best_score_} best_params:{grid.best_params_}')\nmeans = grid.cv_results_['mean_test_score']\nstds = grid.cv_results_['std_test_score']\nparams = grid.cv_results_['params']\n\nfor mean, std, param in zip(means, stds, params):\n    print(f'mean:{mean} - std:{std} - param:{param}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# grid.best_params_.keys()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def list_params():\n\n    # mark these as globals ince values might change\n    # also get error 'Unbound LocalError' \n    global best_optimizer, best_neurons, num_hidden_layers, best_weight_constraint, best_dropout\n    global best_activation, best_init_mode, best_learning_rate, best_momentum, best_epoch_num\n    global best_batch_size\n    \n    # capture best values for each hyper-parameter if evailable in gridCV run \n    if 'optimizer' in grid.best_params_.keys():\n        best_optimizer= grid.best_params_['optimizer']\n        \n    if 'epochs' in grid.best_params_.keys():\n        best_epoch_num = grid.best_params_['epochs']\n\n    if 'batch_size' in grid.best_params_.keys():\n        best_batch_size = grid.best_params_['batch_size']\n\n    if 'lr' in grid.best_params_.keys():\n        best_learning_rate = grid.best_params_['lr']\n\n    if 'momentum' in grid.best_params_.keys():\n        best_momentum = grid.best_params_['momentum']\n\n    if 'init_mode' in grid.best_params_.keys():\n        best_init_mode= grid.best_params_['init_mode']\n\n    if 'activation' in grid.best_params_.keys():\n        best_activation= grid.best_params_['activation']\n\n    if 'dropout' in grid.best_params_.keys():\n        best_dropout = grid.best_params_['dropout']\n\n    if 'neurons' in grid.best_params_.keys():\n        best_neurons = grid.best_params_['neurons']\n\n    if 'weight_constraint' in grid.best_params_.keys():\n        best_weight_constraint = grid.best_params_['weight_constraint']\n\n    if 'num_hidden_layers' in grid.best_params_.keys():\n        best_num_hidden_layers = grid.best_params_['num_hidden_layers']\n        \n    ## add to hyper_params dictionary\n    hyper_params['optimizer'] = best_optimizer\n    hyper_params['dropout'] = best_dropout\n    hyper_params['neurons'] = best_neurons\n    hyper_params['weight_constraint'] = best_weight_constraint\n    hyper_params['activation'] = best_activation\n    hyper_params['init_mode'] = best_init_mode\n    hyper_params['learning_rate'] = best_learning_rate\n    hyper_params['momentum'] = best_momentum\n    hyper_params['epochs'] = best_epoch_num\n    hyper_params['batch_size'] = best_batch_size\n\n    hyper_params['num_hidden_layers'] = num_hidden_layers\n\n    ## print best values\n    print('best_optimizer: ', best_optimizer)\n    print('best_neurons: ', best_neurons)\n    print('num_hidden_layers: ', num_hidden_layers)\n    print('best_weight_constraint: ', best_weight_constraint)\n    print('best_dropout: ', best_dropout)\n    print('best_activation: ', best_activation)\n    print('best_init_mode: ', best_init_mode)\n    print('best_learning_rate: ', best_learning_rate)\n    print('best_momentum: ', best_momentum)\n    print('best_epoch_num: ', best_epoch_num)\n    print('best_batch_size: ', best_batch_size)\n\n\nlist_params()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Create Final Model incorporating all Tuned hyper-parameters**","metadata":{"id":"wTE7wdDkjFzF"}},{"cell_type":"code","source":"train_df.shape, y_train.shape, train_df.shape, train_df.columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(train_df, y_train, test_size=0.3, random_state=111)","metadata":{"id":"uoHJO8wWgDis"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(X_train.shape, X_val.shape, y_train.shape, y_val.shape, test_df.shape)","metadata":{"id":"7tdWKXhklAt-","outputId":"f461d4d2-5f91-483f-a57d-836f10dac5dd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### list of hyper-parameters fine-timed for titanic disaster dataset","metadata":{}},{"cell_type":"code","source":"# list all hyper-parameters for this run\nfor key in hyper_params.keys():\n  print(f'key: {key:20s} value: {hyper_params[key]}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### build and execute final model","metadata":{}},{"cell_type":"markdown","source":"use captured hyperparamters during GridSearchCV\nyou can also manually change values to build and execute the model to improve accuracy, etc..","metadata":{}},{"cell_type":"code","source":"clear_bags(cvmodel)\nfinal_model = create_hyper_model(lr=best_learning_rate,\n                                 momentum=best_momentum,\n                                 init_mode=best_init_mode,\n                                 activation=best_activation,\n                                 dropout=best_dropout,\n                                 weight_constraint=best_weight_constraint,\n                                 neurons=best_neurons,\n                                 optimizer=best_optimizer,\n                                 num_hidden_layers=en(num_hidden_layers),\n                                 inputdf=X_train,\n                                )\nfinal_model.summary()","metadata":{"id":"PITiID0wgDv9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Keras callbacks - Learning Rate, and Early Stopping","metadata":{}},{"cell_type":"markdown","source":"**[Early Stopping](https://keras.io/api/callbacks/early_stopping/)** - stop training when a monitored metric has improved to minimize loss metric.  \nThe qty to be monitored needs to be available in 'logs' dict.","metadata":{}},{"cell_type":"code","source":"# early stopping call back on 'val_loss'\ncallback = keras.callbacks.EarlyStopping(monitor='val_accuracy',  # what to monitor\n                                         min_delta=0.03,      # change to monitor\n                                         patience=35,          # num of epochs to wait before breaking out\n                                         mode='max',          # here 'min' stop decreasing\n                                         verbose=1,           # verbose=0 don't print output\n                                         restore_best_weights=False,   # whether to restore model weights\n                                         )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This function keeps the initial learning rate for the first ten epochs\n# and decreases it exponentially after that.\n#\n# learning rate scheduling\nLEARN_RATE = 0.001\n\ndef lr_scheduler(epoch):\n    global LEARN_RATE\n    if epoch < 10:\n        LEARN_RATE += 1e-7 \n    else:\n        LEARN_RATE *= tf.math.exp(-0.004)\n        \n    return LEARN_RATE\n\n# define callback\nlr_callback = keras.callbacks.LearningRateScheduler(\n    lr_scheduler, \n    verbose=True,\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = final_model.fit(X_train, y_train,\n                          epochs= best_epoch_num,\n                          batch_size = best_batch_size,\n                          validation_data=(X_val, y_val),\n                          callbacks=[callback, lr_callback],\n                          verbose=1,\n                         )\n\n# Training stopped early due to setting of callback\nmodel_eval = final_model.evaluate(X_val, y_val)\nprint(model_eval)","metadata":{"id":"bPRW75e9gD25"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict on validation dataset\nval_pred_model = final_model.predict(X_val)\n\n# print(val_pred_model)\nval_predicted = [0 if pred < 0.5 else 1 for pred in val_pred_model]\nprint('\\nNeural Nets - report on Confusion matrix, Classification report, Accuracy and F1 scores')\nprint(confusion_matrix(y_val, val_predicted))\nprint(classification_report(y_val, val_predicted))\n\nprint(f'Accuracy Score: {accuracy_score(y_val, val_predicted):.2f}')\nprint(f'F1 Score: {f1_score(y_val, val_predicted):.2f}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### loss and accuracy plots","metadata":{"id":"YlOUAnGvgD8F"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nhist_frame=pd.DataFrame(data=history.history)\nplt.figure(figsize=(12,9))\nplt.subplot(2,2,1)\nsns.lineplot(data=(hist_frame.loss, hist_frame.val_loss))\nplt.subplot(2,2,2)\nsns.lineplot(data=(hist_frame.accuracy, hist_frame.val_accuracy))","metadata":{"id":"iX-WXtGggEAp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### predictions test data","metadata":{}},{"cell_type":"code","source":"test_model_predictions = final_model.predict(test_df)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### create submission.csv file","metadata":{}},{"cell_type":"code","source":"# now predict on Test dataset and copy to CSV\n\ntest_model_data = pd.DataFrame()\ntest_model_data['PassengerId'] = passengerId\n\ntest_model_data['Survived'] = [0 if pred < 0.5 else 1 for pred in test_model_predictions]\ntest_model_data.to_csv('submission.csv', index = False)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!head submission.csv","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<u>For more information on hyper-parameter search also chekout:</u><br>\n[Grid Search Hyperparameters for Deep Learning Models - Jason Brownlee](https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/)<br>\n[Tuning the Hyperparameters and Layers of NN Deep Learning - Rendyk](https://www.analyticsvidhya.com/blog/2021/05/tuning-the-hyperparameters-and-layers-of-neural-network-deep-learning/)","metadata":{}},{"cell_type":"markdown","source":"[JBL]:https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras","metadata":{}},{"cell_type":"markdown","source":"[RDK]:https://www.analyticsvidhya.com/blog/2021/05/tuning-the-hyperparameters-and-layers-of-neural-network-deep-learning","metadata":{}}]}