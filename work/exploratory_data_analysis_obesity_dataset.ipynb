{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# DeepLearning Predictions on Multiclass Obesity Risk dataset","metadata":{}},{"cell_type":"markdown","source":"<HR>\n\n[<b>Multi-Class Prediction of Obesity Risk</b>](https://www.kaggle.com/competitions/playground-series-s4e2) dataset.\n\n<hr>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\"><p style =\"font-size:1.3em\">\n<h4>Accuracy will improve if dataset EDA and clean-up adheres to guielines to train <u>Deep Neural Network models</u> (actually not sure yet as to what they really are)</h4>\n</div>\n<p>\n\n**Note:** I tried multiple diffferent ways to clean DataSet to get it in to a shape that can be create clusters(spatials) and improve predictions. The best I could get so far was 88.945. I put comments in appropriate places in Notebook as to what EDA methods I had tried. <br />\n\n**If you are going to try out this notebook, appreciate if you let me know what you did to improve accuracy**\n</p>\n","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n\n<b>Used Keras_Tuner to search for best hyperparameter values - search was over `25` parameters</b><br />\nSince preiction is on non-image dataset, and we only use Fully-Connected Dense layers, used only few layers + this is a Functional Neural Network model:<br />\n\n$$Input -> Dense -> DropOut -> BatchNormalization -> Dense -> DropOut -> Dense(output, sigmoid-activation)$$\n\n[Notebook on Kaggle](https://www.kaggle.com/code/jayyanamandala/keras-tuner-hyperparameters-search-obesiry-risk)\n<ul>\n    <li>batch_size - same is referenced in 'HyperTuningNetwork' class but name is different</li>\n    <li>number of epochs in run</li>\n    <li>Number of Neurons in first fully connected Dense layer</li>\n    <li>Number of Neurons in second fully connected Dense layer</li>\n    <li>drop_rates - for two Dropout layers</li>\n    <li>kernel_regularizers(4), bias_regularizers(4), activity_regularizers(4)</li>\n    <li>layer activation - relu, tanh, sigmoid, ..\n    <li>model optimizer - adam, sgd, ...</li>\n    <li>learning_rate - learning_rate for Model optimizer</li>\n    <li>decay_steps - - learning rate decay steps</li>\n    <li>decay_rate - learning_rate decay</li>\n  </ul>\n\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\"><h1>import packages</h1></div>","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n%matplotlib inline\n\nimport seaborn as sns\nimport warnings\n\nwarnings.filterwarnings('ignore')\n%autosave 60\nfrom datetime import datetime\n\npd.set_option('display.max_columns', None)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport shutil\nimport sys\nfrom glob import glob\nimport re\nimport math\nimport random as py_random   # to differentiate btw Numpy and Python - incase random is set to np.random","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preprocessing and model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, OrdinalEncoder\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, PowerTransformer\n\n# metrics and utils\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.utils import compute_class_weight, compute_sample_weight\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn import set_config\nset_config(display=\"diagram\")\n\nimport scipy","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow.keras as keras\nfrom keras.utils import to_categorical","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        tf.config.set_visible_devices(gpus[1:],'GPU')\n        tf.config.experimental.set_memory_growth(gpus[0], True)\n        print('setting session for memory growth')\n    except:\n        pass","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Notebook uses PowerTransformer scaling - version of scikit-learn package must be higher than 1.2.2\n# !pip install scikit-learn>1.2.2 --upgrade\nimport sklearn as sk\nprint(sk.__version__)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def reset_seeds():\n   np.random.seed(42)\n   py_random.seed(42)\n   tf.random.set_seed(42)\n\n# set a beginning for consistensy\nreset_seeds()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\"><h1>load datasets</h1></div>","metadata":{}},{"cell_type":"code","source":"# download dataset\n# If you are on Kaggle go to competition page and create a notebook\n# -OR- if Kaggle is setup at home, please download dataset\n# !kaggle competitions download -c playground-series-s4e2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/playground-series-s4e2/train.csv')\ntest_df = pd.read_csv('/kaggle/input/playground-series-s4e2/test.csv')\ntrain_cat_cols = train_df.select_dtypes(include=['object', 'category'])\ntest_cat_cols = test_df.select_dtypes(include=['object', 'category'])\ntrain_num_cols = train_df.select_dtypes(exclude=['object', 'category'])\ntest_num_cols = test_df.select_dtypes(exclude=['object', 'category'])\n\n# Drop 'id' column from Train and Test\ntrain_num_cols.drop(['id'], inplace=True, axis=1)\ntest_num_cols.drop(['id'], inplace=True, axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\"><h1>Intial Data Explortion - and cleanup</h1></div>","metadata":{}},{"cell_type":"code","source":"# print columns to check\npd.DataFrame(data=[train_df.columns, test_df.columns]).T.rename({0:'Train', 1:'Test'}, axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\"><h4>Get list of category columns from train and test datasets and check if the unique values match</h4>\n <b>do not check the 'ground truth' column</b>\n\n</div>","metadata":{}},{"cell_type":"code","source":"# check unique values in Category columns - must be equal\ndef check_columns_exist(df1,df2, check_equal=True, traindf = train_df, testdf=test_df):\n    for i in df1.columns:\n        if i in df2.columns:try:\n        if not np.array_equal( np.sort(np.unique(traindf[i])), \n                            np.sort(np.unique(testdf[i]))):\n          print('\\n#####Column:', i , 'elements are not equal ######')\n          print('Train:', np.sort(np.unique(traindf[i])), \n              '\\nTest:', np.sort(np.unique(testdf[i])), end='\\n\\n')\n        else:\n              pass\n              # print('\\nColumn:', i , '\\nTrain:', np.sort(np.unique(traindf[i])), \n              #     '\\nTest:', np.sort(np.unique(testdf[i])))\n      except: \n          pass\n    else:\n      print('\\nColumn:', i , 'does not exist in testdf', end='\\n\\n')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check category columns in Train and Test and also check if categorical elements in each Category are same\ncheck_columns_exist(train_cat_cols, test_cat_cols)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get totals and percentages for each category and kind\ndef get_percent(col, kk, df):\n  # for kk in np.sort(np.unique(df[col])):\n  total = len(df[df[col] == kk])\n  val = (total/len(df)) * 100\n  val = f'{val:.2f}'\n  val=float(val)\n  return total, val\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_totals=[]\ntrain_values=[]\ntrain_categories = []\ntrain_columns = []\n\n# check unique values in Category columns - must be equal\nfor i in train_cat_cols.columns:\n  if i in test_cat_cols.columns:\n    kk = np.sort(np.unique(train_df[i]))\n    for k in kk:\n      # print(i, kk, k)\n      train_columns.append(i)\n      tr_total, tr_val = get_percent(i, k, train_df)\n      train_categories.append(k)\n      train_totals.append(tr_total)\n      train_values.append(tr_val)\n  ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_categories = []\ntest_totals=[]\ntest_values=[]\n\n# check unique values in Category columns - must be equal\nfor i in train_cat_cols.columns:\n  if i in test_cat_cols.columns:\n    kk = np.sort(np.unique(test_df[i]))\n    for k in kk:\n      # print(i, kk, k)\n      tr_total, tr_val = get_percent(i, k, test_df)\n      test_categories.append(k)\n      test_totals.append(tr_total)\n      test_values.append(tr_val)\n  ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create MultiIndex DataFrame\narrays_col_cats = [np.array(train_columns), np.array(train_categories)]\narrays = list(zip(train_totals, train_values, test_totals, test_values))\ndf = pd.DataFrame(arrays, columns=['Train-Totals', 'Train-Values', 'Test-Totals', 'Test-Values'])\ndf.set_index(arrays_col_cats, inplace=True)\n\ndiff1 = abs(df['Train-Values'] - df['Test-Values'])\nsum1 = abs(df['Train-Values'] + df['Test-Values'])\ntr_std = np.std(df['Train-Values'].astype(np.float32))\nte_std = np.std(df['Test-Values'].astype(np.float32))\n\ndf['Diff %'] = round(diff1,2).astype(str) + '%'\ndf['Train-Values'] = df['Train-Values'].astype(str) + '%'\ndf['Test-Values'] = df['Test-Values'].astype(str) + '%'\ndf['Diff/Sum'] = round(np.divide(diff1,sum1) * 100,2)\n\ndf","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h4>from the above stats we can see that the datasets 'Train' and 'Test' are spread approximately equally amongst individual categorical features - extra CALC 'feature' in Test dataset</h4>\n\nfrom the above table, based on Diff/Sum column - we combine the following:\n\n1. Bike + Motobike                   - Two_Wheelers \n2. Public Transportation + Walking   - Non_Motors      (looks the dataset is based on entirely different demographics)\n3. Combine Test 'CALC - Always' with Frequently   and drop 'Always'\n4. Create BMI column and delete Height and Weight from both datasets\n\n","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\"><h4>Create a BMI column and delete Height and Weight from both datasets</h4>","metadata":{}},{"cell_type":"markdown","source":"In most people, BMI correlates to body fat<b>\n[https://my.clevelandclinic.org/health/body/24052-adipose-tissue-body-fat](https://my.clevelandclinic.org/health/body/24052-adipose-tissue-body-fat) </b>\n- the higher the number, the more body fat you have, but according to some clinical studies itâ€™s not accurate in some cases.\n\nFrom the train and test dataset we can infer that the weight is in 'pounds' and height is in feet   \nwe convert height to inches and calculate BMI\n","metadata":{}},{"cell_type":"code","source":"def calc_bmi(x,y): \n\n    # Assuming Height is in Meters and Weight in 'pounds'\n    # USC - ONE -of- x in lbs, and y in inches\n    # x = x * 703;  y = np.square(y); x/y\n\n    # SI -ONE- of - x in kgs, and y in meters\n    # x = x;  y = np.square(y); x/y\n\n    # convert to inches - since weight is in pounds\n    # convert height from meters first to Centimeters \n    # and multiply by 0.394 to convert to inches\n    # calculate BMI and return value\n    return (x * 703 )/np.square(y * 100 * 0.394)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## create BMI columns for train and test datasets - and drop 'Age' 'Height'","metadata":{}},{"cell_type":"code","source":"train_df['BMI'] = train_df.apply(lambda x: calc_bmi(x['Weight'], x['Height']), axis=1)\ntest_df['BMI'] = test_df.apply(lambda x: calc_bmi(x['Weight'], x['Height']), axis=1)\n\n# drop columns 'Weight' and 'Height' from both train_df and test_df create_datasets\ntrain_df.drop(['Weight', 'Height'], axis=1, inplace=True)\ntest_df.drop(['Weight', 'Height'], axis=1, inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\"><h4>Replace 'Public_transportation' & 'Walking' with 'Non_Motors',<br>\nand 'Bike' & 'Motorbike' with 'Two_Wheelers'</h4>","metadata":{}},{"cell_type":"code","source":"test_df['MTRANS'].replace(['Public_Transportation', 'Walking'], 'Non_Motors', inplace=True)\ntrain_df['MTRANS'].replace(['Public_Transportation', 'Walking'], 'Non_Motors', inplace=True)\n\ntest_df['MTRANS'].replace(['Bike', 'Motorbike'], 'Two_Wheelers', inplace=True)\ntrain_df['MTRANS'].replace(['Bike', 'Motorbike'], 'Two_Wheelers', inplace=True)\ntest_df['CALC'].replace(['Always'], 'Frequently', inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# let's recreate columns and numerals lists\ntrain_cat_cols = train_df.select_dtypes(include=['object', 'category'])\ntest_cat_cols = test_df.select_dtypes(include=['object', 'category'])\ntrain_num_cols = train_df.select_dtypes(exclude=['object', 'category'])\ntest_num_cols = test_df.select_dtypes(exclude=['object', 'category'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\"><h4>check duplicates and NAs in train and test datasets</h4></div>","metadata":{}},{"cell_type":"code","source":"# ref: https://www.kaggle.com/code/nnjjpp/pipelines-for-preprocessing-a-tutorial\ntrain_df.duplicated().sum()\npd.DataFrame([train_df.duplicated().sum(), \n           test_df.duplicated().sum()]).T.rename({0:'Train', \n                                                  1:'Test'}, \n                                                 axis=1).rename(index={0: '# of Duplicates'})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check NA values\npd.concat([train_df.isna().sum(0), \n           test_df.isna().sum(0)], \n          axis=1).T.rename(index={0:'Train', \n                          1:'Test'})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print columns\npd.DataFrame([train_cat_cols.columns, test_cat_cols.columns, \n              train_num_cols.columns, test_num_cols.columns, \n             ]).T.rename({0:'Train Cat', 1:'Test Cat', 2:'Train Num', 3:'Test Num'}, axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\"><h1>plots</h1></div>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\"><p style =\"font-size:1.2em\">Outliers Detection</p></div>","metadata":{}},{"cell_type":"code","source":"num_rows =  2\nplt.figure(figsize=(num_rows*10,4))\n\nplt.suptitle('Age/BMI Box Plots')\nplt.subplots_adjust(hspace=0.7)\nplt.subplot(num_rows,2,1)\nplt.boxplot(train_df.Age, vert=False)\nplt.ylabel('Variable')\nplt.xlabel('Age')\nplt.title('Train Age Box Plot')\n\nplt.subplot(num_rows,2,2)\nplt.boxplot(train_df.BMI, vert=False)\nplt.ylabel('Variable')\nplt.xlabel('BMI')\nplt.title('Train BMI Box Plot')\n\nplt.subplot(num_rows,2,3)\nplt.boxplot(test_df.Age, vert=False)\nplt.ylabel('Variable')\nplt.xlabel('Age')\nplt.title('Test Age Box Plot')\n\nplt.subplot(num_rows,2,4)\nplt.boxplot(test_df.BMI, vert=False)\nplt.ylabel('Variable')\nplt.xlabel('BMI')\nplt.title('Test BMI Box Plot')\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the above boxplots for 'Age' we can see that the spread for Test and Train datasets seems similar.<br>\n**we will look into 'Age' column later**","metadata":{}},{"cell_type":"code","source":"# create a new dataframe to show boxplots between numerals and:\n# family_history_with_overweight\n# SMOKE\n# Gender\ntrain_gender_df = pd.concat([train_df[train_num_cols.columns],  \n                             train_df['Gender'].to_frame(),\n                             train_df['family_history_with_overweight'].to_frame(),\n                             train_df['SMOKE'].to_frame(),\n                            ], \n                            axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_gender_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_rows=1\nplt.figure(figsize=(num_rows*14,4))\nplt.suptitle('Age/BMI against family_history_with_overweight Box Plots')\nplt.subplots_adjust(hspace=0.7)\nplt.subplot(num_rows,2,1)\nsns.boxplot( x=\"Age\", y='family_history_with_overweight', data=train_gender_df, )\nplt.subplot(num_rows,2,2)\nsns.boxplot( x=\"BMI\", y='family_history_with_overweight', data=train_gender_df, )\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"form the above boxplot we see that we have a lot of data of young people with no family_history_with_overweight\n- data is not distributed between yes & no of family_history_with_overweight for Age - skewed\n- <b><p style =\"color:red; font-size:1.2em\">we will not discard'Age', but create bins and convert to categorical</p></b>","metadata":{}},{"cell_type":"code","source":"train_gender_df = pd.concat([train_df[train_num_cols.columns],  \n                             train_df['Gender'].to_frame(),\n                             train_df['family_history_with_overweight'].to_frame(),\n                             train_df['SMOKE'].to_frame(),\n                            ], \n                            axis=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_rows=3\nplt.figure(figsize=(num_rows*12,25))\nplt.suptitle('Age/BMI against family_history_with_overweight/SMOKE/Gender Box Plots')\nplt.subplots_adjust(hspace=0.7)\nplt.subplot(num_rows,2,1)\nsns.boxplot( x=\"Age\", y='family_history_with_overweight', data=train_gender_df, )\nplt.subplot(num_rows,2,2)\nsns.boxplot( x=\"BMI\", y='family_history_with_overweight', data=train_gender_df, )\n\nplt.subplot(num_rows,2,3)\nsns.boxplot( x=\"Age\", y='SMOKE', data=train_gender_df, )\nplt.subplot(num_rows,2,4)\nsns.boxplot( x=\"BMI\", y='SMOKE', data=train_gender_df, )\n\nplt.subplot(num_rows,2,5)\nsns.boxplot( x=\"Age\", y='Gender', data=train_gender_df, )\nplt.subplot(num_rows,2,6)\nsns.boxplot( x=\"BMI\", y='Gender', data=train_gender_df, )\n\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style =\"font-style:bold; font-size:1.4em; color:red\">Bin BMI and convert to  categories</p>\n**follow the sequence listed**: <br />\n1. bin  <br />\n2. create labels  <br />\n3. run qcut firs time without labels  <br />\n4. run qcut again with labels  <br />","metadata":{}},{"cell_type":"code","source":"# let's bin BMI - we have 7 classes\nnum_bins = 7\nbmi1 = ((train_df.BMI//num_bins)*num_bins).min()\nbmi2 = ((train_df.BMI//num_bins+1)*num_bins).max()\n\nbmi_bins = np.arange(bmi1,bmi2+num_bins,num_bins)\nbmi_labels = ['bmi_'+str(round(f)) for f in np.arange(bmi1,bmi2+num_bins,num_bins)]\nbmi_labels, bmi_bins","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<p style =\"font-style:bold; font-size:1.4em; color:red\">Bin Age and convert to  categories</p>","metadata":{}},{"cell_type":"code","source":"# let's bin Age - we have 7 classes\nnum_bins = 7\nage1 = ((train_df.Age//num_bins)*num_bins).min()\nage2 = ((train_df.Age//num_bins+1)*num_bins).max()\n\nage_bins = np.arange(age1,age2+num_bins,num_bins)\nage_labels = ['age_'+str(round(f)) for f in np.arange(age1,age2+num_bins,num_bins)]\nage_labels, age_bins","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We use same bins for Training and Test datasets","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.shape, train_df.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.shape, train_df.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['BMI_bins'] = pd.qcut(train_df.BMI, q=len(bmi_bins), duplicates='drop' )\ntrain_df['BMI_bins'] = pd.qcut(train_df.BMI, q=len(bmi_bins),  labels=bmi_labels, duplicates='drop' )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['BMI_bins'] = pd.qcut(test_df.BMI, q=len(bmi_bins), duplicates='drop' )\ntest_df['BMI_bins'] = pd.qcut(test_df.BMI, q=len(bmi_bins),  labels=bmi_labels, duplicates='drop' )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['Age_bins'] = pd.qcut(train_df.Age, q=len(age_bins), duplicates='drop' )\ntrain_df['Age_bins'] = pd.qcut(train_df.Age, q=len(age_bins),  labels=age_labels, duplicates='drop' )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['Age_bins'] = pd.qcut(test_df.Age, q=len(age_bins), duplicates='drop' )\ntest_df['Age_bins'] = pd.qcut(test_df.Age, q=len(age_bins),  labels=age_labels, duplicates='drop' )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.shape, train_df.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.drop(['Age'], axis=1, inplace=True)\ntrain_df.drop(['Age'], axis=1, inplace=True)\ntest_df.drop(['BMI'], axis=1, inplace=True)\ntrain_df.drop(['BMI'], axis=1, inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# categorical\ntrain_cat_cols = train_df.select_dtypes(include=['object', 'category'])\ntest_cat_cols = test_df.select_dtypes(include=['object', 'category'])\n\n# numerical\ntrain_num_cols = train_df.select_dtypes(exclude=['object', 'category'])\ntest_num_cols = test_df.select_dtypes(exclude=['object', 'category'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot all features\nplt.figure(figsize=(3,3))\nplt.subplots_adjust(hspace=0.4)\ncols = 3\nrows=2\nfig,ax = plt.subplots(nrows=rows,ncols=cols,figsize=(20,20))\nax = ax.flatten()\nplt.rcParams[\"axes.labelsize\"] = 10\ntruth_label='NObeyesdad'\nplt.suptitle(\"Distributions of Multi-Class Obesity dataset\\n\",size=24)\n# textprops={'fontsize': 16},\nxx = 0\nfor i,col in enumerate(train_df.columns):\n    if col not in train_cat_cols.columns:\n      # print('1',col)\n        if col == 'id':    # do not plot 'id'column\n            continue\n        else:\n            sns.histplot(data=train_df,x=col,ax=ax[xx],kde=True,line_kws={\"ls\":\"-\"})\n            xx += 1\n\nplt.tight_layout()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\"><h4>function to split trainXY and test_X</h4></div>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-warning\"><h4>In the function below from the train and test datasets we will drop 'id'</h4>","metadata":{}},{"cell_type":"code","source":"train_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_datasets(trainxy, testx):\n  # capture test_ids\n  submit_id = testx['id']\n  \n  # Remove 'id' from dataseta\n  testx = testx.drop(['id'], axis=1)\n  trainx = trainxy.drop(['id'], axis=1)\n  \n  return trainx, testx, submit_id\n  ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create training sets train_X, train_Y, test_X, test_id \n# remove 'id' cp;umn from Train and Test\n# train_X, train_Y,  test_X, test_id = create_datasets(train_df, test_df)\ntrain_X, test_X, test_id = create_datasets(train_df, test_df)\n\n# recapture columns list - category, and numerical\n# \n# categorical\ntrain_cat_cols = train_df.select_dtypes(include=['object', 'category'])\ntest_cat_cols = test_df.select_dtypes(include=['object', 'category'])\n\n# numerical\ntrain_num_cols = train_df.select_dtypes(exclude=['object', 'category'])\ntest_num_cols = test_df.select_dtypes(exclude=['object', 'category'])\n\n# drop some columns - id, NObeyesdad from list\ntrain_num_cols.drop(['id'], inplace=True, axis=1)\ntest_num_cols.drop(['id'], inplace=True, axis=1)\ntrain_cat_cols.drop(['NObeyesdad'], inplace=True, axis=1)\n\n# show columns\npd.DataFrame([train_num_cols.columns, test_num_cols.columns]).rename(index={0:'Train', 1:'Test'})\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\"><p style =\"font-size:1.2em\">Create train_Y</p></div>","metadata":{}},{"cell_type":"code","source":"train_Y = train_X.NObeyesdad\ntrain_X.drop(['NObeyesdad'], axis=1, inplace=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check NA values\npd.concat([train_X.isna().sum(0), \n           test_X.isna().sum(0)], \n          axis=1).T.rename(index={0:'Train', \n                          1:'Test'})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\"><h1>plot categorical columns</h1></div>","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(22,22))\nplt.rcParams[\"axes.labelsize\"] = 20\nrows, num = 3, 3\ncols = 0\n\n# ref: https://stackoverflow.com/questions/63687789/how-do-i-create-a-pie-chart-using-categorical-data-in-matplotlib\ndef label_function(val):\n  return f'{val / 100 * len(train_cat_cols):.0f}\\n{val:.0f}%'   # returns nums and percent\n  # return f'{val:.0f}%'\n\nfor n in range(rows):\n  for i in range(num):\n    plt.subplot(3,3,cols+1)\n    if len(test_cat_cols.columns) > cols:\n      train_cat_cols.groupby(train_cat_cols.columns[cols]).size().plot(kind='pie', \n                                                                     autopct=label_function, \n                                                                     textprops={'fontsize': 16},\n                                                                     colormap='prism_r'\n                                                                    )\n      plt.title(train_cat_cols.columns[cols])\n    cols += 1\n    plt.axis('off')\n  \n  ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(22,22))\nplt.rcParams[\"axes.labelsize\"] = 20\nrows, num = 3, 3\ncols = 0\n\n# ref: https://stackoverflow.com/questions/63687789/how-do-i-create-a-pie-chart-using-categorical-data-in-matplotlib\ndef label_function(val):\n  return f'{val / 100 * len(test_cat_cols):.0f}\\n{val:.0f}%'   # returns nums and percent\n  # return f'{val:.0f}%'\n\nfor n in range(rows):\n  for i in range(num):\n    plt.subplot(3,3,cols+1)\n    if len(test_cat_cols.columns) > cols:\n      test_cat_cols.groupby(test_cat_cols.columns[cols]).size().plot(kind='pie', \n                                                                     autopct=label_function, \n                                                                     textprops={'fontsize': 16},\n                                                                     colormap='plasma_r'\n                                                                    )\n      plt.title(test_cat_cols.columns[cols])\n    cols += 1\n    plt.axis('off')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\"><h1>Final Data Explortion - one last look</h1></div>","metadata":{}},{"cell_type":"code","source":"type(train_X), train_X.shape, train_X.columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(test_X), test_X.shape, test_X.columns","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_X.describe().transpose()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_X.info(show_counts=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(train_X.isnull().sum() !=0)\nprint ('No Null values in train dataset') if not 1 in train_X.isnull().sum() else print(train_X.isnull().sum())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(test_X.isnull().sum() !=0)\nprint ('No Null values in test dataset') if not 1 in test_X.isnull().sum() else print(test_X.isnull().sum())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Datasets - scaling and encoding","metadata":{}},{"cell_type":"markdown","source":"## Scikit-Learn PowerTransformer scaling technique","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\"><h4>\nFor Fully Connected Neural Networks when used to predict majority class for classification problems or predict values against continuous data<br>\n**Depending on EDA** analysis, and how data is cleaned, sklearn scaler packages seem to react differently since the spread of numerals is different<br />\n<br><u>PowerTransformer</u> scaling seem to work best in some cases - but Accuracy of validation set never seem to cross 89.945%\n<br><u>StandardScaled and/or MinMaxScaled</u> I think StandardScaler did okin last few runs\n\n<br>The prediction -or- inference against testset (Kaggle competition) synthetically created <u>rose by <b>14.2</b> percentage point</u>\n  \n</h4></div>","metadata":{}},{"cell_type":"code","source":"data_tr = train_X.copy()\ndata_te = test_X.copy()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labelEnc = LabelEncoder()\ny_encoded = labelEnc.fit_transform(train_Y)\n\ny = tf.keras.utils.to_categorical(y_encoded)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(y[:10])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# categorical  \ncat_col_ = train_X.select_dtypes(include=['object', 'category']).columns \ntrain_cat_cols = train_X.select_dtypes(include=['object', 'category'])   \ntest_cat_cols = test_X.select_dtypes(include=['object', 'category'])\nprint('categorical columns:', cat_col_)                                                                                                                                                                                                                                               \n\n# numerical                                                                                                                                \nnum_col_ = train_X.select_dtypes(exclude=['object', 'category']).columns                                                                   \ntrain_num_cols = train_X.select_dtypes(exclude=['object', 'category'])  \ntest_num_cols = test_X.select_dtypes(exclude=['object', 'category'])                                                                       \nprint('numerical columns:', num_col_)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_X = pd.get_dummies(test_X, dtype=int)\ntrain_X = pd.get_dummies(train_X, dtype=int)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(train_X), train_X.shape, type(test_X), test_X.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scaler = StandardScaler()                                                                                                                  train_X[num_col_] = scaler.fit_transform(train_X[num_col_])\ntest_X[num_col_]  = scaler.transform(test_X[num_col_])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\"><p style =\"font-size:1.3em\">EnScaling</div>","metadata":{}},{"cell_type":"code","source":"X.shape, type(X), X_test.shape, type(X_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## end different scaling technique","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# split datasets into three - training, val, and hold_out","metadata":{"execution":{"iopub.status.busy":"2024-02-23T11:22:44.057256Z","iopub.execute_input":"2024-02-23T11:22:44.057713Z","iopub.status.idle":"2024-02-23T11:22:44.093263Z","shell.execute_reply.started":"2024-02-23T11:22:44.057677Z","shell.execute_reply":"2024-02-23T11:22:44.092364Z"}}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\"><h4>\nWe split the dataset into three sets: <b>Train, Validation, and Test</b>.<br><br>\nAll 3 come from the same stream, but only Train/Validation are used for training and evaluation.<br>\nWe will use 'Test' to check predictions and graph confusion matrix (sns.heatmap)\n</h4></div>","metadata":{}},{"cell_type":"code","source":"# split data befo augmentation\ntrainX, valX, trainY, valY = train_test_split(X, y,\n                                              test_size=0.2,    # split 15% for validation & test\n                                              shuffle=True,\n                                              random_state=42)\n\nvalX, testX, valY, testY = train_test_split(valX, valY,\n                                            test_size=0.3,    # spit  30% for test and 70% for validation\n                                            shuffle=True,\n                                            random_state=42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\"><h4>\n<b>Ref:</b>\n\n[barbagrande007](https://www.kaggle.com/code/barbagrande007/bbg007-s4e2-obesity)\n\n<br>Add jittering. Introduce noise to X to increase data size, similar to image augmentation techniques in Convolutional Neural Networks.\n  <ul>\n  <li>Creates more training samples\n  <li>decreases overfitting\n  <li>improves accuracy and predictability\n  </ul>\n  \n</h4></div>","metadata":{}},{"cell_type":"markdown","source":"# Augment - Datasets for Analysis and Prediction","metadata":{}},{"cell_type":"code","source":"dng1 = np.random.default_rng(seed=42)\ndng2 = np.random.default_rng(seed=46)\ndng3 = np.random.default_rng(seed=142)\ndng4 = np.random.default_rng(seed=146)\ndng5 = np.random.default_rng(seed=16)\ndng6 = np.random.default_rng(seed=66)\ndng7 = np.random.default_rng(seed=116)\ndng8 = np.random.default_rng(seed=166)\n\nX_jitter1 = trainX + dng1.random(1) * 0.3\nX_jitter2 = trainX + dng2.random(1) * 0.3\nX_jitter3 = trainX + dng3.random(1) * 0.3\nX_jitter4 = trainX + dng4.random(1) * 0.3\nX_jitter5 = trainX + dng5.random(1) * 0.3\nX_jitter6 = trainX + dng6.random(1) * 0.3\nX_jitter7 = trainX + dng7.random(1) * 0.3\nX_jitter8 = trainX + dng8.random(1) * 0.3\n\n# Duplicate X, y - COMMENT OUT those we don't need\ntrainX = np.vstack((trainX,\n                    X_jitter1, \n                    X_jitter2, \n                    X_jitter3, \n#                X_jitter4, \n#               X_jitter5, \n#               X_jitter6, \n#               X_jitter7,\n#               X_jitter8,\n              ))\n\ntrainY = np.vstack((trainY,\n                    trainY, # 1\n                    trainY, # 2\n                    trainY, # 3 \n#               trainY, # 4 \n#               trainY, # 5 \n#               trainY, # 6 \n#               trainY, # 7 \n#               trainY, # 8 \n              ))\n\n# Randomize samples\nshuffled_indices = np.random.permutation(len(X))\ntrainX = trainX[shuffled_indices]\ntrainY = trainY[shuffled_indices]\n\n# delete jitter arrays\ndel X_jitter1, X_jitter2, X_jitter3, X_jitter4\ndel X_jitter5, X_jitter6, X_jitter7, X_jitter8","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# check shapes of all 3 sets\nprint(f'Train: X:{trainX.shape} Y:{trainY.shape}')\nprint(f'Val  : X:{valX.shape}   Y:{valY.shape}')\nprint(f'Test : X:{testX.shape}  Y:{testY.shape}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Deep Neural Network - Tensorflow -> Keras","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\"><h4>\nImported packages - most are not used in this Notebook, similar setup can be used to Keras_Tune hyperparameters for Convolutional Neural Networks\n</h4></div>","metadata":{}},{"cell_type":"code","source":"import tensorflow.keras as keras\nimport keras.backend as K\n\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization\nfrom tensorflow.keras.layers import Input\nfrom keras.layers import ReLU, LeakyReLU\n\nfrom keras.models import Model, Sequential\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, LearningRateScheduler\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\nfrom  keras.regularizers import L1 , L2, L1L2\nimport  keras.regularizers as regularizers\nfrom keras.optimizers import Adam, SGD\nfrom tensorflow.keras.backend import clear_session","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\"><h1>model callbacks</h1></div>","metadata":{}},{"cell_type":"code","source":"# ModelCheckpoint\nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n  filepath='obesity.hd5',\n  save_weights_only=True,         # only save weights\n  monitor='val_accuracy',\n  mode='max',\n  save_best_only=True,\n)\n\n# Reduce Learning Rate\n# Giving ERROR when enabled - doesn't work when assigning Learning_Rate to Adam\nreduce_lr = ReduceLROnPlateau(\n  monitor='val_loss',\n  factor=0.04,\n  patience=5,\n  min_lr=0.0,\n)\n\n# Early Stopping\nearly_stop = EarlyStopping(\n  monitor='val_loss',\n  mode='auto',\n  verbose=0,\n  patience=3,\n)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class My_Callback(keras.callbacks.Callback):\n    def on_epoch_begin(self, epoch, logs={}):\n      self.epoch = epoch\n\n    def on_batch_end(self, batch, logs={}):\n        # if self.epoch == 10 and batch == 3:\n        if self.epoch == 10:\n          print (f\"\\nStopping at Epoch {self.epoch}, Batch {batch}\")\n          self.model.stop_training = True\n\n    # def on_epoch_end(self, epoch, logs={}):\n    #     if self.epoch == 20:\n    #        print (f\"\\nStopping at Epoch {self.epoch}\")\n    #      # cannot access self.model.stop_training in this function - check source","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\"><h4>\nThe function below is a stand-alone function and uses the following <b>hyperparameters</b> to tune Neural Network to improve validation accuracy and decrease training and validation loss.<br>\n  <ul>\n    <li>batch_size - same is referenced in 'HyperTuningNetwork' class but name is different</li>\n    <li>epochs - number of epochs in run</li>\n    <li>layer1 - Number of Neurons in first fully connected Dense layer</li>\n    <li>layer2 - A float to scale the number of neurons in layer1 and use as Number of Neurons in second fully connected Dense layer</li>\n    <li>l2_reg - Float - value of Kernel_regularizer - L2</li>\n    <li>learning_rate - To setup learning_rate_scheduler for Adam - Model optimizer</li>\n    <li>decay_steps - To setup learning_rate_scheduler for Adam - Model optimizer</li>\n    <li>decay_rate - To setup learning_rate_scheduler for Adam - Model optimizer</li>\n  </ul>\n</h4></div>","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\"><h1>create & build model</h1></div>","metadata":{}},{"cell_type":"code","source":"# Functional Model\ndef build_keras_model():\n    keras.backend.clear_session()       # turn-off and check how Keras_Tuner behaves\n \n    # setup up parameter search values for batch_size\n    # This is also setup in 'HyperTuningNetwork' class - 'fit' function\n    tr_shape = trainX.shape[1]                     # don't really - using to shorten name\n    num_classes = trainY.shape[1]                  # same reason as above\n\n  # Tune the number of units in the first Dense layer\n  # base number is '560'   - min is 450, max i 700\n    hp_units1 = 1520  # 980\n\n  # For layer2 units, use a scaling factor based on # of Neurons in first layer\n  hp_units2 = 190  # 750 \n  \n  # setting conditional hyperparameters\n  # https://github.com/keras-team/keras-tuner/issues/66\n  # a = hp.Int('a', 0, 10)\n  # with hp.conditionaLscope('a', 5):\n  #   b = hp.Int('b', 0, 10)\n\n\n  # drop rates\n    hp_drop1 = 0.44\n    hp_drop2 = 0.71\n\n\n  # Input\n  inp = Input(shape=(trainX.shape[1],))\n \n  #### ONE ####\n    stage1 = Dense(units=1520,\n                 activation='relu',\n                 kernel_regularizer=L2(l2=,|0.000144)\n                 bias_regularizer=L2(l2=0.00849)\n                 activity_regularizer=,\n                 kernel_initializer = 'glorot uniform'),\n                 )(inp)\n\n    drop1 = Dropout(hp_drop1)(stage1)\n    batch1 = BatchNormalization()(drop1)\n\n  #### TWO ####\n  stage2 = Dense(units = 190,\n                 activation='relu',\n                 kernel_regularizer=L1(l1=0.0004999)\n                 bias_regularizer=L2(l2=0.00098332)\n                 activity_regularizer=L1(l1=0.00010774),\n                 kernel_initializer='glorot uniform',\n                )(batch1)\n    drop2 = Dropout(hp_drop2)(stage2)\n\n  #### OUT ####\n  outp = Dense(num_classes, activation='softmax',)(drop2)\n\n{  ##################################################################\n  # To setup learning_rate_scheduler for Adam - Model optimizer\n  hp_learning_rate = 0.0003450908797091988      # 0.0004122296936091948      # 0.0005776234810416469 # 0.0010388011586892568\n  hp_decay_steps =   43510.0                    # 39990.0                    # 30570.0               # 19990\n  hp_decay_rate =    0.14500871466470402        # 0.36073213824416583        # 0.28642696462807177    @ 0.3132430029454445\n\n  ##################################################################\n  # tensorflow.keras.optimizers.schedules.ExponentialDecay\n  lr_schedule = ExponentialDecay(initial_learning_rate=hp_learning_rate, \n                                 decay_steps=hp_decay_steps,\n                                 decay_rate=hp_decay_rate,\n                                )\n\n  ## opt = Adam(learning_rate=hp_learning_rate)\n  # opt = Adam(learning_rate=lr_schedule)\n  optimizer = 'adam' # 'adamax'\n\n  # get optimizer from tensorflow.keras.optimizers baseclass\n  opt=keras.optimizers.get(optimizer)\n  # learning_rate = hp.Choice(\"learning_rate\", values=[0.01, 0.1])\n  learning_rate = lr_schedule\n  opt.learning_rate=lr_schedule    # setup learning_rate\n  model = Model(inp, outp)\n  model.compile(loss = 'categorical_crossentropy',\n                # optimizer=optimizer,\n                optimizer=opt,\n                metrics=['accuracy'],\n               )\n  return model}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# build model\nepochs = 480     \nbatch_size = 2000\n\n# select between using valX, valY or a subset of training data as validation\nUSE_VAL_BATCH_DATA = True\n\n# build model and print summary\nmodel = build_keras_model()\n\nmodel.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\"><h3>\nfirst epoch run - stop after epoch 10 completes\n</h3></div>","metadata":{}},{"cell_type":"code","source":"history = model.fit(trainX, trainY, \n                    epochs = epochs, \n                    batch_size = batch_size, \n                    validation_data=(valX,valY) if USE_VAL_BATCH_DATA else None,\n                    validation_split=0.3 if not USE_VAL_BATCH_DATA else None,\n                    callbacks=[model_checkpoint_callback, My_Callback()],\n                    verbose=2,\n                   )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n<h2>plot loss and accuracy - first run</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"hist_frame=pd.DataFrame(data=history.history)\n\nplt.figure(figsize=(8,4))\nplt.subplot(1,2,1)\nsns.lineplot(data=(hist_frame.loss, hist_frame.val_loss))\nplt.title('Loss')\nplt.subplot(1,2,2)\nsns.lineplot(data=(hist_frame.accuracy, hist_frame.val_accuracy))\nplt.title('Accuracy')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\"><h3>\nsecond epoch run - start from epoch 11\n</h3></div>","metadata":{}},{"cell_type":"code","source":"use_early_stop = True\nhistory = model.fit(trainX, trainY, \n                    epochs = epochs, \n                    batch_size = batch_size, \n                    validation_data=(valX,valY) if USE_VAL_BATCH_DATA else None,\n                    validation_split=0.3 if not USE_VAL_BATCH_DATA else None,\n                    callbacks=[model_checkpoint_callback, early_stop] if use_early_stop else [model_checkpoint_callback],\n                    initial_epoch=11,\n                    verbose=2,\n                   )\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n<h2>plot loss and accuracy - second run</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"hist_frame=pd.DataFrame(data=history.history)\n\nplt.figure(figsize=(8,4))\nplt.subplot(1,2,1)\nsns.lineplot(data=(hist_frame.loss, hist_frame.val_loss))\nplt.title('Loss')\nplt.subplot(1,2,2)\nsns.lineplot(data=(hist_frame.accuracy, hist_frame.val_accuracy))\nplt.title('Accuracy')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\"><h1>model evaluate (validation) and predict (hold-out)</h1></div>","metadata":{}},{"cell_type":"code","source":"model.evaluate(valX, valY, batch_size=32)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_predictions = model.predict(valX)\nv_predictions=[]\nfor i in range(len(val_predictions)):\n  # print(\"Predicted=%s\" % np.argmax(val_predictions[i]))\n  v_predictions.append(np.argmax(val_predictions[i]))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert testY to true_labels\nvalY_actual=[]\nfor i in range(len(valY)):\n  valY_actual.append(np.argmax(valY[i]))\n\nunique_nums = np.unique([valY_actual, v_predictions])\nunique_label = labelEnc.inverse_transform(unique_nums)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(confusion_matrix(valY_actual, \n                             v_predictions), \n            annot=True, \n            cmap='viridis', \n            fmt='d', \n            xticklabels=unique_label,\n            yticklabels=unique_label,\n            square=True)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix - Validation Set')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# classification report\nprint(classification_report(valY_actual, v_predictions))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# confusion_matrix\nprint(confusion_matrix(valY_actual, v_predictions))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\"><h1>Predictions - Hold-Out set</h1></div>","metadata":{}},{"cell_type":"code","source":"test_predictions = model.predict(testX)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions=[]\nfor i in range(len(test_predictions)):\n  # print(\"Predicted=%s\" % np.argmax(test_predictions[i]))\n  predictions.append(np.argmax(test_predictions[i]))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert testY to true_labels\ntestY_actual=[]\nfor i in range(len(testY)):\n  testY_actual.append(np.argmax(testY[i]))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unique_nums = np.unique([testY_actual, predictions])\nunique_label = labelEnc.inverse_transform(unique_nums)\nsns.heatmap(confusion_matrix(testY_actual, predictions), \n            xticklabels=unique_label, \n            yticklabels=unique_label,\n            annot=True, \n            cmap='viridis', \n            fmt='d', \n            square=True)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix - Test (HoldOut Set')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# classification report\nprint(classification_report(testY_actual, predictions))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# confusion_matrix\nprint(confusion_matrix(testY_actual, predictions))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\"><h1>inference - Multi-Class Prediction (Obesity Risk)</h1></div>","metadata":{}},{"cell_type":"code","source":"# predictions_ = model.predict(X_test_cluster)\npredictions_ = model.predict(X_test)\n\nprint(predictions_[:5])\n\npredictions_max=[]\nfor i in range(len(predictions_)):\n  predictions_max.append(np.argmax(predictions_[i]))\n\n# Inverse label encoder\npredictions_submit = labelEnc.inverse_transform(predictions_max)\nprint(predictions_submit[:5])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_name = 'submission_88945.csv'\nsubmit_pd = pd.read_csv(file_name)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert to onehot-encoding\nsubmit_encoded = labelEnc.fit_transform(submit_pd.iloc[:,1])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# heatmap\nsns.heatmap(confusion_matrix(submit_encoded, predictions_max), \n            xticklabels=unique_label, \n            yticklabels=unique_label,\n            annot=True, \n            cmap='viridis', \n            fmt='d', \n            square=True)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# classification report\nprint(classification_report(submit_encoded, predictions_max))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# confusion_matrix\nconfusion_matrix(submit_encoded, predictions_max)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"curr = datetime.now()\ncurr = curr.strftime('%y_%m_%d_%H_%M_%S')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create DataFrame to write CSV file\n# del predictions_data\npredictions_data = pd.DataFrame(predictions_submit, columns=['NObeyesdad'])\npredictions_data.insert(0, 'id', test_id)\nhold_submission = f'submission_{curr}.csv'\npredictions_data.to_csv(hold_submission, index = False)\npredictions_data.to_csv('submission.csv', index = False)\n\npredictions_data.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!head -10 submission.csv","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## end inference","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-success\">\n<h4>submit your CSV file</h4>\n<div>","metadata":{}},{"cell_type":"code","source":"stop","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"curr = f'Multi-Class Prediction (Obesity Risk) submitted:  {curr}'\nprint(curr)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!kaggle competitions submit -c playground-series-s4e2 -f submission.csv -m curr","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<block><pre>\n@misc{playground-series-s4e2,\n    author = {Walter Reade, Ashley Chow},\n    title = {Multi-Class Prediction of Obesity Risk},\n    publisher = {Kaggle},\n    year = {2024},\n    url = {https://kaggle.com/competitions/playground-series-s4e2}\n}\n</block></pre>","metadata":{}},{"cell_type":"markdown","source":"<block><pre>\n@misc{omalley2019kerastuner,\n    title        = {KerasTuner},\n    author       = {O'Malley, Tom and Bursztein, Elie and Long, James and Chollet, Fran\\c{c}ois and Jin, Haifeng and Invernizzi, Luca and others},\n    year         = 2019,\n    howpublished = {\\url{https://github.com/keras-team/keras-tuner}}\n}\n</block></pre>","metadata":{}}]}